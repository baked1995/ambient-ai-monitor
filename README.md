# üéß Ambient AI Sound Monitor (Jetson LAN)

An **Edge AI‚Äìbased audio intelligence platform** designed for **sound recognition, speaker identification, and ambient monitoring**, optimized for **NVIDIA Jetson devices** and local LAN usage.

This system enables **browser-based audio capture**, **structured dataset creation**, and **CNN-based sound recognition** without sending audio to the cloud.

---

## üöÄ Key Capabilities

- üéô **Browser microphone recording** (no device mic required on Jetson)
- üìÅ **Structured training data collection**
- üì§ **Recognition-only audio uploads**
- üß† **Pre-trained CNN-based audio recognition (YAMNet)**
- üìä **Waveform, loudness, and spectrogram visualization**
- üß© **Recording profiles for high-quality dataset collection**
- üîí **LAN-only, privacy-preserving architecture**

---

## üß± System Architecture
Browser (User Device)
‚îÇ
‚îÇ (Web Audio API)
‚ñº
Streamlit UI (Jetson)
‚îÇ
‚îú‚îÄ‚îÄ Audio Visualization
‚îú‚îÄ‚îÄ Recording Profiles
‚îú‚îÄ‚îÄ Dataset Management
‚îÇ
‚ñº
FastAPI Backend
‚îÇ
‚îú‚îÄ‚îÄ Training Data Storage
‚îú‚îÄ‚îÄ Recognition Uploads
‚îÇ
‚ñº
Audio Intelligence Engine
‚îú‚îÄ‚îÄ YAMNet (CNN)
‚îî‚îÄ‚îÄ Feature Extraction

## üìÇ Project Structure
ambient-ai-monitor/
‚îú‚îÄ‚îÄ app/
‚îÇ ‚îú‚îÄ‚îÄ streamlit_app.py # UI & visualization
‚îÇ ‚îú‚îÄ‚îÄ api.py # Audio ingestion API
‚îÇ ‚îú‚îÄ‚îÄ browser_mic.html # Web Audio capture
‚îÇ
‚îú‚îÄ‚îÄ model/
‚îÇ ‚îú‚îÄ‚îÄ yamnet_utils.py # Model helpers (future-ready)
‚îÇ
‚îú‚îÄ‚îÄ data/ # Runtime data (git-ignored)
‚îÇ ‚îú‚îÄ‚îÄ recordings/
‚îÇ ‚îú‚îÄ‚îÄ training/
‚îÇ ‚îî‚îÄ‚îÄ recognition/
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md

> ‚ö†Ô∏è **Audio data is intentionally excluded from Git** to ensure privacy and clean ML workflows.

---

## üß† Model Lifecycle & Intelligence Pipeline

### Phase 1 ‚Äî Sound Understanding (Current)

The system uses **YAMNet**, a **CNN-based audio event classifier** trained on Google‚Äôs **AudioSet** (over 2M labeled sound clips).

**Model details:**
- Architecture: **Convolutional Neural Network (CNN)**
- Input: Raw audio waveform (16 kHz mono)
- Output: Probability scores across **521 sound classes**
- Examples:
  - Human speech
  - Keyboard typing
  - Switch clicks
  - Water, wind, ambient noise

**Why YAMNet?**
- Lightweight
- Proven accuracy
- Ideal for edge devices like Jetson
- No training required initially

---

### Phase 2 ‚Äî Dataset Quality & Recording Profiles (Implemented)

To ensure **high-quality training data**, the system introduces **Recording Profiles**:

| Profile | Purpose |
|------|------|
| Voice (Speaker ID) | Speaker identification datasets |
| Keyboard | Keystroke sound modeling |
| Switch / Button | Mechanical sound detection |
| Ambient Noise | Background baselining |

Each profile provides **human-readable capture guidance**, improving:
- Signal consistency
- Label accuracy
- Model performance later

---

### Phase 3 ‚Äî Feature Intelligence (Implemented)

For every audio clip, the system computes:

- **RMS Energy** ‚Üí loudness consistency
- **Zero Crossing Rate (ZCR)** ‚Üí noise / sharpness
- **Spectral Centroid** ‚Üí frequency distribution
- **Spectrogram (Log-frequency)** ‚Üí time‚Äìfrequency behavior

These features help:
- Detect poor recordings
- Identify noisy or silent samples
- Validate training data before ML training

---

### Phase 4 ‚Äî Speaker Embeddings (Planned)

Next step will introduce **speaker embeddings**:

- Convert voice samples into **fixed-length vectors**
- Compare voices using cosine similarity
- Enable:
  - Speaker verification
  - Voice clustering
  - Unknown speaker detection

This builds on Phase 1 without retraining from scratch.

---

### Phase 5 ‚Äî Custom Classifier Training (Planned)

Final phase introduces **custom ML models** trained on your collected data:

- Use YAMNet embeddings as input
- Train:
  - Speaker classifiers
  - Environment-specific sound detectors
- Models optimized for:
  - Jetson Orin Nano
  - Real-time inference
  - Offline operation

---

## üîê Privacy & Security

- Audio never leaves the local network
- HTTPS enforced via NGINX
- No cloud dependency
- No third-party data sharing

---

## üß™ Ideal Use Cases

- Smart environments
- Industrial monitoring
- Healthcare ambient sensing
- Voice-based access systems
- Edge AI research & prototyping

---

## üìå Technology Stack

- **Frontend:** Streamlit
- **Audio Capture:** Web Audio API
- **Backend:** FastAPI
- **ML Model:** YAMNet (TensorFlow Hub)
- **Visualization:** Librosa + Matplotlib
- **Deployment:** NVIDIA Jetson (Edge AI)

---

## üìà Roadmap

- [x] Phase 1: Audio understanding
- [x] Phase 2: Recording profiles
- [x] Phase 3: Feature analytics
- [ ] Phase 4: Speaker embeddings
- [ ] Phase 5: Custom classifiers
- [ ] Phase 6: Anomaly detection

---

## üìÑ License

Internal / Client-specific usage  
(Contact project owner for redistribution)
